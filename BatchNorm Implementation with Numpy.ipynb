{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量归一化(Batch Normalizaition) 在 CNN 设计中起到举足轻重的作用, 使得较深的神经网络训练变得更容易, 并能够使用更大的学习率加快模型收敛速度. 基于 DL 框架 (如 MXNet, PyTorch, Tensorflow *et.al.*) 的应用通常将其作为层直接嵌入网络结构, 并没有深入研究具体实现. 本文旨在通过无需框架依赖的源码实现了解其背后的实质. \n",
    "\n",
    "参考链接:   \n",
    "- https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html  \n",
    "- https://wiseodd.github.io/techblog/2016/07/04/batchnorm/  \n",
    "- https://zh.d2l.ai/chapter_convolutional-neural-networks/batch-norm.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import nn\n",
    "from utils import *\n",
    "\n",
    "class BatchNorm(nn.Block):\n",
    "    def __init__(self, num_features, num_dims, **kwargs):\n",
    "        super(BatchNorm, self).__init__(**kwargs)\n",
    "        self.eps = 1.0e-10\n",
    "        self.momentum = 0.9\n",
    "        shape = (1, num_features) if num_dims == 2 else (1, num_features, 1, 1)\n",
    "\n",
    "        self.gamma = self.params.get('gamma', shape=shape, init=init.One())\n",
    "        self.beta = self.params.get('beta', shape=shape, init=init.Zero())\n",
    "\n",
    "        self.moving_mean = nd.zeros(shape)\n",
    "        self.moving_var = nd.zeros(shape)\n",
    "\n",
    "    def forward(self, X,  is_training=True):\n",
    "        if self.moving_mean.context != X.context:\n",
    "            self.moving_mean = self.moving_mean.copyto(X.context)\n",
    "            self.moving_var = self.moving_var.copyto(X.context)\n",
    "\n",
    "        # inference only\n",
    "        if not is_training:\n",
    "            X_hat = (X - self.moving_mean) / nd.sqrt(self.moving_var + self.eps)\n",
    "\n",
    "        # training\n",
    "        else:\n",
    "            assert len(X.shape) in (2, 4)\n",
    "\n",
    "            # case of full connected layer\n",
    "            if len(X.shape) == 2:\n",
    "                mean = X.mean(axis=0)\n",
    "                var = ((X - mean) ** 2).mean(axis=0)\n",
    "\n",
    "            # case of convolution layer\n",
    "            if len(X.shape) == 4:\n",
    "                mean = X.mean(axis=(0, 2, 3), keepdims=True)\n",
    "                var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)\n",
    "            X_hat = (X - mean) / nd.sqrt(var + self.eps)\n",
    "\n",
    "            # update moving_mean and moving_var\n",
    "            self.moving_mean = self.momentum * self.moving_mean + (1.0 - self.momentum) * mean\n",
    "            self.moving_var = self.momentum * self.moving_var + (1.0 - self.momentum) * var\n",
    "\n",
    "        Y = self.gamma.data() * X_hat + self.beta.data()\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "这里我们构建 LeNet, 替换 Gluon 模块中的 nn.BatchNorm() 为自定义 BatchNorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lenet():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Conv2D(6, kernel_size=5),\n",
    "            BatchNorm(6, num_dims=4),\n",
    "            nn.Activation('sigmoid'),\n",
    "            nn.MaxPool2D(pool_size=2, strides=2),\n",
    "            nn.Conv2D(16, kernel_size=5),\n",
    "            BatchNorm(16, num_dims=4),\n",
    "            nn.Activation('sigmoid'),\n",
    "            nn.MaxPool2D(pool_size=2, strides=2),\n",
    "            nn.Dense(120),\n",
    "            BatchNorm(120, num_dims=2),\n",
    "            nn.Activation('sigmoid'),\n",
    "            nn.Dense(84),\n",
    "            BatchNorm(84, num_dims=2),\n",
    "            nn.Activation('sigmoid'),\n",
    "            nn.Dense(10))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1\tloss=0.5418\ttrain-acc=0.807\ttest-acc=0.857\ttime=19.6 sec\n",
      "epoch=2\tloss=0.3589\ttrain-acc=0.868\ttest-acc=0.873\ttime=17.0 sec\n",
      "epoch=3\tloss=0.3209\ttrain-acc=0.882\ttest-acc=0.880\ttime=17.4 sec\n",
      "epoch=4\tloss=0.2999\ttrain-acc=0.889\ttest-acc=0.884\ttime=16.5 sec\n",
      "epoch=5\tloss=0.2828\ttrain-acc=0.896\ttest-acc=0.900\ttime=16.6 sec\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    net = Lenet()\n",
    "    net.initialize(ctx=ctx, init=init.Xavier())\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx=ctx)\n",
    "        print('epoch=%d\\tloss=%.4f\\ttrain-acc=%.3f\\ttest-acc=%.3f\\ttime=%.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc, time.time() - start))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lr = 1.0\n",
    "    num_epochs = 5\n",
    "    batch_size = 32\n",
    "    ctx = mx.gpu(0)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
